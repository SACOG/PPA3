{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d94e299-9815-46c3-9047-9f383933cc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules loaded.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib\n",
    "import datetime as dt\n",
    "from time import perf_counter\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import sqlalchemy as sqla\n",
    "\n",
    "import arcpy\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor\n",
    "\n",
    "def get_odbc_driver():\n",
    "    # gets name of ODBC driver, with name \"ODBC Driver <version> for SQL Server\"\n",
    "    drivers = [d for d in pyodbc.drivers() if 'ODBC Driver ' in d]\n",
    "    \n",
    "    if len(drivers) == 0:\n",
    "        errmsg = f\"ERROR. No usable ODBC Driver found for SQL Server.\" \\\n",
    "        f\"drivers found include {drivers}. Check ODBC Administrator program\" \\\n",
    "        \"for more information.\"\n",
    "        \n",
    "        raise Exception (errmsg)\n",
    "    else:\n",
    "        d_versions = [re.findall('\\d+', dv)[0] for dv in drivers] # [re.findall('\\d+', dv)[0] for dv in drivers]\n",
    "        latest_version = max([int(v) for v in d_versions])\n",
    "        driver = f\"ODBC Driver {latest_version} for SQL Server\"\n",
    "    \n",
    "        return driver\n",
    "\n",
    "driver = get_odbc_driver()\n",
    "\n",
    "print(\"modules loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46480b88-b75e-4f74-83c8-b3765b6c19cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs specified. Ready to create table.\n"
     ]
    }
   ],
   "source": [
    "# BASED ON TESTING, THIS IS A STILL SLOW BUT FASTER THAN ESRI CHUNK OF CODE TO RUN\n",
    "# A SQL QUERY AND LOAD DIRECTLY INTO AN ESRI FEATURE CLASS\n",
    "\n",
    "#-------------SPECIFY WHICH TABLES FOR DIFFERENT YEARS-----------------\n",
    "ilut_tbl = 'ilut_combined2035_177_DPS'  # \"ilut_combined2020_63_DPS\"\n",
    "eto_tbl = 'raw_eto2035_DPS_latest' # 'raw_eto2020_DPS_latest'\n",
    "data_year = 2035 # 2035 or 2020\n",
    "\n",
    "output_gdb = r'I:\\Projects\\Darren\\PPA3_GIS\\PPA3_GIS.gdb'  # r'I:\\SDE_Connections\\SDE-PPA\\owner@PPA.sde'  # r'I:\\Projects\\Darren\\PPA3_GIS\\PPA3_GIS.gdb'\n",
    "\n",
    "\n",
    "#-----------SPECIFY COLUMNS TO USE (SHOULD NOT VARY WITH YEAR)\n",
    "jnkey_ilut = 'PARCELID'\n",
    "pclpt_x = 'XCOORD'\n",
    "pclpt_y = 'YCOORD'\n",
    "ilut_cols = [jnkey_ilut, pclpt_x, pclpt_y, 'GISAc', 'JURIS', 'County', 'DU',\n",
    "             'POP_TOT', 'HH_hh', 'ENR_K12', 'EMPTOT', 'EMPFOOD', 'EMPRET', 'EMPSVC', 'EMPIND', \n",
    "            'PT_TOT_RES', 'SOV_TOT_RES', 'HOV_TOT_RES', 'TRN_TOT_RES', 'BIK_TOT_RES', 'WLK_TOT_RES', \n",
    "             'VMT_TOT_RES']\n",
    "\n",
    "jnkey_eto = 'PARCELID'\n",
    "eto_cols = ['LU'] # exclude join key because you do not want duplicate columns\n",
    "\n",
    "#------------OTHER SPECIFICATIONS----\n",
    "assert_types = {pclpt_x: 'float', pclpt_y: 'float', 'GISAc': 'float'}\n",
    "rename_dict = {'DU': 'DU_TOT', 'LU': 'LUTYPE'}\n",
    "\n",
    "\n",
    "#------------BUILD QUERY--------------\n",
    "ilut_cols = ', '.join(f\"ilut.{fname}\" for fname in ilut_cols)\n",
    "eto_cols = ', '.join(f\"ilut.{fname}\" for fname in eto_cols)\n",
    "\n",
    "query_str = f\"\"\"SELECT\n",
    "    {ilut_cols},\n",
    "    {eto_cols}\n",
    "    FROM {ilut_tbl} ilut\n",
    "        JOIN {eto_tbl} eto\n",
    "            ON ilut.{jnkey_ilut} = eto.{jnkey_eto}\"\"\"\n",
    "\n",
    "lutag = ilut_tbl.split('ilut_combined')[1]\n",
    "out_tbl_name = f\"ppa_pclpnt{lutag}\"\n",
    "output_tbl = str(Path(output_gdb).joinpath(f\"parcel_data_pts_{data_year}\"))\n",
    "\n",
    "print(\"outputs specified. Ready to create table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab62398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df chunk iterator created.\n",
      "creating feature class I:\\Projects\\Darren\\PPA3_GIS\\PPA3_GIS.gdb\\parcel_data_pts_2035...\n",
      "loading rows...\n",
      "\tupdated field JURIS to accommodate longer string value.\n",
      "\t100000 rows loaded...\n",
      "\t200000 rows loaded...\n",
      "\t300000 rows loaded...\n",
      "\t400000 rows loaded...\n",
      "\t500000 rows loaded...\n",
      "\t600000 rows loaded...\n",
      "\t700000 rows loaded...\n",
      "\t800000 rows loaded...\n",
      "830280 rows inserted in 7.4 mins.\n"
     ]
    }
   ],
   "source": [
    "#---------CREATE ITERABLE DATAFRAME WITH CHUNKS----------------\n",
    "servername = 'SQL-SVR'\n",
    "dbname = 'MTP2024'\n",
    "trustedconn = 'yes'\n",
    "\n",
    "conn_str = f\"DRIVER={driver};\" \\\n",
    "        f\"SERVER={servername};\" \\\n",
    "        f\"DATABASE={dbname};\" \\\n",
    "        f\"Trusted_Connection={trustedconn}\"\n",
    "\n",
    "conn_str = urllib.parse.quote_plus(conn_str)\n",
    "engine = sqla.create_engine(f\"mssql+pyodbc:///?odbc_connect={conn_str}\")\n",
    "\n",
    "df_itr = pd.read_sql_query(sql=query_str, con=engine, chunksize=1000)\n",
    "\n",
    "print(\"df chunk iterator created.\")\n",
    "\n",
    "#------GO THROUGH CHUNKS AND LOAD INTO FEATURE CLASS-----------------\n",
    "st = perf_counter()\n",
    "\n",
    "rowcnt = 0\n",
    "for i, chunk in enumerate(df_itr):\n",
    "    rowcnt += chunk.shape[0]\n",
    "    \n",
    "    chunk = chunk.rename(columns=rename_dict)\n",
    "    for fn, dtyp in assert_types.items():\n",
    "        chunk[fn] = chunk[fn].astype(dtyp)\n",
    "        \n",
    "    chunk_s = pd.DataFrame.spatial.from_xy(chunk, x_column=pclpt_x, y_column=pclpt_y, sr=2226)\n",
    "    if i == 0:\n",
    "        print(f\"creating feature class {output_tbl}...\")\n",
    "        chunk_s.spatial.to_featureclass(output_tbl, sanitize_columns=False)\n",
    "        \n",
    "        out_tbl_fnames = [f.name for f in arcpy.ListFields(output_tbl)]\n",
    "        fields_to_use = [f for f in out_tbl_fnames if f in chunk.columns]\n",
    "        fields_to_use.append('SHAPE@XY')\n",
    "        print(\"loading rows...\")\n",
    "    else:\n",
    "        drecs = chunk.to_dict(orient='records')\n",
    "        with arcpy.da.InsertCursor(output_tbl, field_names=fields_to_use) as inscur:\n",
    "            for rec in drecs:\n",
    "                try:\n",
    "                    coords = (rec[pclpt_x], rec[pclpt_y])\n",
    "                    row = [rec[fname] for fname in out_tbl_fnames if fname in fields_to_use] # put into correct output order\n",
    "                    row.append(coords)\n",
    "                    inscur.insertRow(row)\n",
    "                except RuntimeError:\n",
    "                    vlengths = {fname: len(v) for fname, v in rec.items() if isinstance(v, str)} # length of string vals in current row\n",
    "                    fclengths = {f.name: f.length for f in arcpy.ListFields(output_tbl) \\\n",
    "                                 if f.type == 'String'} # defined field lengths in feature class\n",
    "                    try:\n",
    "                        for f, fc_flen in fclengths.items():\n",
    "                            rowvlen = vlengths.get(f)\n",
    "                            if rowvlen and rowvlen > fc_flen: # if needed, update the fc field length to accommodate the new string val\n",
    "                                arcpy.management.AlterField(output_tbl, field=f, field_length=rowvlen)\n",
    "                                print(f\"\\tupdated field {f} to accommodate longer string value.\")\n",
    "                        inscur.insertRow(row)\n",
    "                    except:\n",
    "                        import pdb; pdb.set_trace()\n",
    "                    \n",
    "                    continue\n",
    "                    \n",
    "        if rowcnt % 100_000 == 0:\n",
    "            print(f\"\\t{rowcnt} rows loaded...\")\n",
    "                \n",
    "elapsed = round((perf_counter() - st) / 60, 1)\n",
    "print(f\"{rowcnt} rows inserted in {elapsed} mins.\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d35cb14a-99df-407d-98a0-aebd0377d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating EJ area tags for parcels...\n",
      "updated EJ tags.\n"
     ]
    }
   ],
   "source": [
    "# SPATIAL JOIN EJ DATA WITH FILTER APPLIED\n",
    "\n",
    "pcltbl = output_tbl\n",
    "pcl_ej_field = 'EJ_AREA'\n",
    "ej_layer = r'I:\\Projects\\Darren\\PPA3_GIS\\PPA3_GIS.gdb\\EJ_2025_final'\n",
    "\n",
    "fl_ej = 'fl_ej'\n",
    "fl_pcl = 'fl_pcl'\n",
    "\n",
    "for fl in [fl_ej, fl_pcl]:\n",
    "    if arcpy.Exists(fl): arcpy.management.Delete(fl)\n",
    "\n",
    "ej_fields = ['EJ_Label', 'Notes']\n",
    "arcpy.management.MakeFeatureLayer(ej_layer, fl_ej)\n",
    "\n",
    "ej_filter = \"EJ_Label <> 'Minority' Or Notes = 'Equity Priority Area'\"\n",
    "arcpy.management.SelectLayerByAttribute(fl_ej, where_clause=ej_filter)\n",
    "\n",
    "print(\"updating EJ area tags for parcels...\")\n",
    "if pcl_ej_field not in [f.name for f in arcpy.ListFields(pcltbl)]:\n",
    "    arcpy.management.AddField(pcltbl, pcl_ej_field, field_type='SHORT')\n",
    "\n",
    "# set default to not be EJ area (0)\n",
    "with arcpy.da.UpdateCursor(pcltbl, [pcl_ej_field]) as ucur:\n",
    "    for row in ucur:\n",
    "        row[0] = 0 # 1 = EJ area\n",
    "        ucur.updateRow(row)\n",
    "        \n",
    "# then for parcels within EJ areas, set EJ=1\n",
    "arcpy.management.MakeFeatureLayer(pcltbl, fl_pcl)\n",
    "arcpy.management.SelectLayerByLocation(fl_pcl, overlap_type='WITHIN', select_features=fl_ej)\n",
    "with arcpy.da.UpdateCursor(fl_pcl, [pcl_ej_field]) as ucur:\n",
    "    for row in ucur:\n",
    "        row[0] = 1 # 1 = is EJ area\n",
    "        ucur.updateRow(row)\n",
    "        \n",
    "arcpy.management.SelectLayerByAttribute(fl_pcl, selection_type='CLEAR_SELECTION')\n",
    "arcpy.management.SelectLayerByAttribute(fl_ej, selection_type='CLEAR_SELECTION')\n",
    "\n",
    "print(\"updated EJ tags.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3d3bdf-e542-4fd2-99c7-9db77f3b923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating base parcel poly layer...\n",
      "adding fields {pt_fields_to_add} poly layer...\n",
      "populating fields {pt_fields_to_add} poly layer...\n",
      "polygon layer created: I:\\Projects\\Darren\\PPA3_GIS\\PPA3_GIS.gdb\\parcel_data_polys_2035\n"
     ]
    }
   ],
   "source": [
    "# MAKE PARCEL POLYGON LAYER FOR PPA\n",
    "arcpy.env.overwriteOutput = True\n",
    "base_polys = r'Q:\\2024_MTPSCS_LandUse\\4_Discussion Scenario\\June2024_DiscusssionScenario_Handoff\\June2024_DiscussionScenario_Handoff.gdb\\DS_2050_June20204'\n",
    "pt_data = r'I:\\SDE_Connections\\SDE-PPA\\owner@PPA.sde\\OWNER.parcel_data_pts_2020'\n",
    "\n",
    "# --------------create base poly layer with just geometry and parcelid\n",
    "print(\"creating base parcel poly layer...\")\n",
    "poly_fc_name = Path(output_tbl).name.replace(\"pts\", \"polys\")\n",
    "dest_polys = str(Path(output_tbl).parent.joinpath(poly_fc_name))\n",
    "\n",
    "include_fields = ['PARCELID', 'Shape']\n",
    "\n",
    "fieldinfo = arcpy.FieldInfo()\n",
    "for field in arcpy.ListFields(base_polys):\n",
    "    fname = field.name\n",
    "    if fname in include_fields:\n",
    "        # addField(field_name, new_field_name, visible, split_rule)\n",
    "        fieldinfo.addField(fname, fname, 'VISIBLE', 'NONE')\n",
    "    else:\n",
    "        fieldinfo.addField(fname, fname, 'HIDDEN', 'NONE')\n",
    "\n",
    "fl_polys = 'fl_polys'\n",
    "if arcpy.Exists(fl_polys): arcpy.Delete_management(fl_polys)\n",
    "arcpy.management.MakeFeatureLayer(base_polys, fl_polys, field_info=fieldinfo)\n",
    "arcpy.conversion.ExportFeatures(in_features=fl_polys, out_features=dest_polys)\n",
    "\n",
    "\n",
    "# ------add appropriate fields from parcel point layer---------\n",
    "pt_fields_to_add = ['LUTYPE', 'VMT_TOT_RES']\n",
    "print(\"adding fields {pt_fields_to_add} poly layer...\")\n",
    "\n",
    "starting_poly_fields = [f.name for f in arcpy.ListFields(dest_polys)]\n",
    "for f in arcpy.ListFields(pt_data):\n",
    "    if f.name in pt_fields_to_add and f.name not in starting_poly_fields:\n",
    "        arcpy.management.AddField(dest_polys, field_name=f.name, field_type=f.type, field_length=f.length)\n",
    "\n",
    "# load data-to-join into dict\n",
    "print(\"populating fields {pt_fields_to_add} poly layer...\")\n",
    "data_to_join = {}\n",
    "pt_field_names = ['PARCELID', *pt_fields_to_add]\n",
    "with arcpy.da.SearchCursor(pt_data, field_names=pt_field_names) as scur:\n",
    "    for row in scur:\n",
    "        data = [row[pt_field_names.index(fname)] for fname in pt_fields_to_add]\n",
    "        data_to_join[row[pt_field_names.index('PARCELID')]] = data\n",
    "\n",
    "# transfer from dict into appropirate fields of poly table\n",
    "with arcpy.da.UpdateCursor(dest_polys, field_names=pt_field_names) as ucur:\n",
    "    for row in ucur:\n",
    "        pclid = round(row[pt_field_names.index('PARCELID')]) # need to round because some are x.9999\n",
    "        datarow = data_to_join.get(pclid)\n",
    "        if datarow:\n",
    "            new_data = [pclid, *datarow]\n",
    "            ucur.updateRow(new_data)\n",
    "\n",
    "print(f\"polygon layer created: {dest_polys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43f90dc-89ba-4173-b6fc-854a777e7f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OBJECTID': 'OID',\n",
       " 'Shape': 'Geometry',\n",
       " 'PARCELID': 'Double',\n",
       " 'Shape_Length': 'Double',\n",
       " 'Shape_Area': 'Double',\n",
       " 'VMT_TOT_RES': 'Double',\n",
       " 'LUTYPE': 'String'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{f.name: f.type for f in arcpy.ListFields(dest_polys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f9acd05-1207-40c8-9c17-e64a091067da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied results over to I:\\SDE_Connections\\SDE-PPA\\owner@PPA.sde\\parcel_data_pts_2035 in 2.2 mins.\n",
      "Copied results over to I:\\SDE_Connections\\SDE-PPA\\owner@PPA.sde\\parcel_data_polys_2035 in 2.8 mins.\n"
     ]
    }
   ],
   "source": [
    "# copy over to PPA SDE\n",
    "def copy_to_sde(in_fc, sde_path):\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    st = perf_counter()\n",
    "    final_output_tbl = str(Path(sde_path).joinpath(Path(in_fc).name))\n",
    "    arcpy.conversion.ExportFeatures(in_fc, final_output_tbl)\n",
    "    elapsed = round((perf_counter() - st) / 60, 1)\n",
    "    print(f\"Copied results over to {final_output_tbl} in {elapsed} mins.\") \n",
    "\n",
    "sde_locn = r'I:\\SDE_Connections\\SDE-PPA\\owner@PPA.sde'\n",
    "\n",
    "copy_to_sde(pcltbl, sde_locn)\n",
    "copy_to_sde(dest_polys, sde_locn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbab93a2-7f3d-4ef1-9860-d0fd453b75c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I:\\\\Projects\\\\Darren\\\\PPA3_GIS\\\\PPA3_GIS.gdb\\\\parcel_data_polys_2035'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_polys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
